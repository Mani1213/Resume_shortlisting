{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPenSGXmSTMNjOLPmikd4Xi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mani1213/Resume_shortlisting/blob/main/Resume_shortlisting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install -q openai langchain\n",
        "pip install --upgrade pillow\n",
        "pip install --upgrade fastai pdfplumber pikepdf torchvision\n",
        "pip install --upgrade torch==2.2.0\n",
        "pip install --upgrade torchaudio torchdata torchtext\n",
        "pip install -q \"unstructured[pdf]\"\n",
        "pip install -q chromadb\n",
        "pip install -q tiktoken\n",
        "pip install -q langchain_experimental\n",
        "\n",
        "pip install unstructured"
      ],
      "metadata": {
        "id": "uoaY0tS5R0CM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q openai langchain\n",
        "! pip install unstructured\n",
        "! pip install --upgrade numpy\n",
        "! pip install -q \"unstructured[pdf]\"\n",
        "! pip install -q chromadb\n",
        "! pip install -q tiktoken\n",
        "! pip install -q langchain_experimental"
      ],
      "metadata": {
        "id": "x61j2bp5SQw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -q pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mu7us5sXWTYx",
        "outputId": "ef77ebf7-920d-43e6-90ec-3dafecd0016e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ],
      "metadata": {
        "id": "tjudqknzA8P_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6542bFVBQtE",
        "outputId": "3c990f20-e531-4744-b877-290d2e9fc9f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install -q kaggle\n",
        "mkdir ~/.kaggle\n",
        "cp /content/drive/MyDrive/kaggle.json ~/.kaggle/\n",
        "chmod 600 ~/.kaggle/kaggle.json\n",
        "kaggle datasets download -d snehaanbhawal/resume-dataset\n",
        "mkdir cv_read\n",
        "unzip /content/resume-dataset.zip -d cv_read"
      ],
      "metadata": {
        "id": "-qt7gzhJCBZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load documents from a folder consisting of multiple resume PDF file using DirectoryLoader"
      ],
      "metadata": {
        "id": "dupWzoUjO3AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = '/content/cv_read/data/data/ENGINEERING/'\n",
        "def load_docs(directory):\n",
        "  loader = DirectoryLoader(directory,show_progress=True) #unstructuredloader by default has used this auto identify file type and load it, mode=\"single\", strategy='fast'(other option is strategy='hi_res' that use yolo varient if mode is elements)\n",
        "  documents = loader.load()\n",
        "  return documents\n",
        "\n",
        "documents = load_docs(directory)\n",
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx1MRx8BC27C",
        "outputId": "9a64b082-b3bd-4666-a6da-786e3df60eda"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/118 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "100%|██████████| 118/118 [01:51<00:00,  1.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "118"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documents=documents[:10]"
      ],
      "metadata": {
        "id": "Dsx4FA8PDxfQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  Split the documents using RecursiveCharacterTextSplitter\n",
        "  with chunk size = 1000, chunk overlap=100\n",
        "* Set the environment with OPENAI_API_KEY\n",
        "* Create OpenAIEmbeddings instance\n",
        "\n"
      ],
      "metadata": {
        "id": "zqleiNh9PVra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_docs(documents, chunk_size=1000, chunk_overlap=100):\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  return docs\n",
        "\n",
        "docs = split_docs(documents)\n",
        "print(len(docs))\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=userdata.get('OPENAI_API_KEY'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTcXNN3fJunA",
        "outputId": "e91fc8bb-c214-4998-dca0-4a8192bc38ba"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Let’s use Chroma DB from langchain vector stores to embed and store the vector database on local hard disk\n",
        "* Now load the persisted database from disk\n",
        "* Initiate the Chroma vector database retriever with k=2, where default search type is similarity_score_threshold\n",
        "* The above retriever helps us to retrieve document section from PDF’s which is suitable for give role"
      ],
      "metadata": {
        "id": "wJ8EF-7zP1NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=docs,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)\n",
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None\n",
        "\n",
        "# Now we can load the persisted database from disk, and use it as normal.\n",
        "vectordb = Chroma(persist_directory=persist_directory,\n",
        "                  embedding_function=embedding)\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2}) # by default search_type=\"similarity_score_threshold\""
      ],
      "metadata": {
        "id": "oJOOPyX4Khaf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the resumes loaded before had skills and other information related to Machine learning role, so it extracted contents of that resume"
      ],
      "metadata": {
        "id": "LPDXteGSTqpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(\"which candidate is good fit for Machine learning engineer roles.\")\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjU6ziEKLG5t",
        "outputId": "dbd20c5e-6d1d-4a50-d4e4-67d052333df8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENGINEERING INTERN Skills\n",
            "\n",
            "C++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\n",
            "\n",
            "Experience ENGINEERING INTERN 08/2016 ï¼​ 12/2016 Company Name State\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* One of the resumes loaded before had skills and other information related to Data Analyst role, so it extracted contents of that resume"
      ],
      "metadata": {
        "id": "xMzNdoLsT7WQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.invoke(\"Give name of candidate who is good fit for Data Analyst roles.\")\n",
        "\n",
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErXmTG51LgHY",
        "outputId": "59e9d978-125f-4b5a-a76c-26a9855d14bb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzed publicly available data from a website. Developed models for optimizing the availability of police officers. Used Excel and AMPL for solving these models and scheduling the officers based on each model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create a ChatOpenAI instance with temperature=0 with model as ‘gpt-3.5-turbo’\n",
        "* Initiate a RetrievalQA chain with ChatOpenAI instance created in last step and with following config : chain_type=”stuff”(means we just stuff entire data into context to pass to LLM. But other methods include Map_reduce,Refine and Map_rerank), retriever (created in previous steps)"
      ],
      "metadata": {
        "id": "ZzTKh3BEUAga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)\n",
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BazntzmXLlTz",
        "outputId": "e2039ff2-45f3-4ec3-cf46-2e0f0d4def7f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Write a function to post process LLM response, Usually in the response of LLM it has meta data i.e source of a document from it answered. So we should capture that detail and print it."
      ],
      "metadata": {
        "id": "7x6vf10qUMx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
        "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
        "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
        "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
        "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
        "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
        "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
        "Strong communication and collaboration skills\"\n",
        "question = warning+job_description + \" Based on the given job description\"\n",
        "query = question + \"short list resumes which is good fit based on skills,education and work experience mwntioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
        "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hK-lUQlLxYv",
        "outputId": "a27fa3fe-cb54-46ff-b703-8c257b940b3f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry, but I cannot generate a shortlist of resumes or provide candidate names based on the given job description.\n",
            "\n",
            "\n",
            "Sources:\n",
            "/content/cv_read/data/data/ENGINEERING/50328713.pdf\n",
            "/content/cv_read/data/data/ENGINEERING/35389360.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Now lets create a prompt which contains a warning so LLM would not hallucinate, job description of some job from LinkedIn and message to short list resumes which is good fit based on skills, education and work experience mentioned in it\n",
        "* Finally run function returned from LLM to retrieve pdf name(i.e resume file name)"
      ],
      "metadata": {
        "id": "0a-YSVyVUXGm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval QA chain."
      ],
      "metadata": {
        "id": "lkvwebNBWg7C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
        "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
        "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
        "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
        "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
        "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
        "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
        "Strong communication and collaboration skills\"\n",
        "question = warning+job_description + \" Based on the given job description\"\n",
        "query = question + \"short list resumes which is good fit based on skills,education and work experience mwntioned in it? also provide the candidate name which will be mentioned in first line of pdf without subheading\"\n",
        "# query = \"short list resumes which is good fit for Data analysis roles based on skills,education and work experience mwntioned in it?\"\n",
        "\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQsgZ_BMWTL2",
        "outputId": "5f20601c-ccda-4f2d-823f-be56d336dca7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have the specific information needed to provide a shortlist of resumes based on the given job description.\n",
            "\n",
            "\n",
            "Sources:\n",
            "/content/cv_read/data/data/ENGINEERING/50328713.pdf\n",
            "/content/cv_read/data/data/ENGINEERING/35389360.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieving resume content which is suitable based on given job description"
      ],
      "metadata": {
        "id": "f8tpi5NeWsnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warning = \"If you don't know the answer, just say that you don't know, don't try to make up an answer\"\n",
        "job_description = \"MS or PhD in computer science or a related technical field,5+ years of industry work experience. Good sense of product with a focus on shipping user-facing data-driven features, Expertise in Python and Python based ML/DL and Data Science frameworks. \\\n",
        "Excellent coding, analysis, and problem-solving skills. Proven knowledge of data structure and algorithms. \\\n",
        "Familiarity in relevant machine learning frameworks and packages such as Tensorflow, PyTorch and HuggingFace\\\n",
        "Experience working with Product Management and decomposing feature requirements into technical work items to ship products\\\n",
        "Experience with generative AI, knowledge of ML Ops and ML services is a plus. This includes Pinecone, LangChain, Weights and Biases etc. \\\n",
        "Familiarity with deployment technologies such as Docker, Kubernetes and Triton are a plus\\\n",
        "Strong communication and collaboration skills\"\n",
        "question = warning+job_description + \" Based on the given job description\"\n",
        "query = question + \"retrive the full document information of a resume which is good fit based on skills,education and work experience mwntioned in it? \"\n",
        "\n",
        "resume_doc = retriever.invoke(query)\n",
        "\n",
        "resume_doc = resume_doc[0].page_content\n",
        "print(resume_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpFV3AXBMAWR",
        "outputId": "f996c69c-d076-4adf-c197-55c3e51e72a4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENGINEERING INTERN Skills\n",
            "\n",
            "C++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\n",
            "\n",
            "Experience ENGINEERING INTERN 08/2016 ï¼​ 12/2016 Company Name State\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Demonstrating the use of prompt template:\n",
        "  Define a prompt template to extract skills, education, projects, publications, work experience as comma separated python list."
      ],
      "metadata": {
        "id": "Up3JHYhzVKe4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_template = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "Skills: what are the technical and non technical skills? \\\n",
        "Answer output them as a comma separated Python list.\n",
        "\n",
        "Education: What is the highest education of the candidate and what is the GPA as mentioned in the text?\\\n",
        "Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\n",
        "\n",
        "Projects: Extract all project titles mentioned in a text\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Publications: Extract all publication titles mentioned in a text\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "Skills\n",
        "Education\n",
        "Projects\n",
        "Publications\n",
        "Work experience\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
        "print(prompt_template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79_TexvKMTLP",
        "outputId": "f7de976e-3df4-466e-9d4c-93daf904fc10"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_variables=['text'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nPublications: Extract all publication titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nProjects\\nPublications\\nWork experience\\n\\ntext: {text}\\n'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Create a conversation chain with memory**\n",
        "Memory type used here is ConversationBufferWindowMemory"
      ],
      "metadata": {
        "id": "PiL_7VnvXeB6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory(k=1)\n",
        "memory.save_context({\"input\": \"Hi\"},\n",
        "                    {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
        "                    {\"output\": \"Cool\"})\n",
        "memory.load_memory_variables({})\n",
        "\n",
        "turbo_llm_memory = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")\n",
        "\n",
        "\n",
        "memory_llm_conversation = ConversationChain(\n",
        "    llm=turbo_llm_memory,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "messages = prompt_template.format_messages(text=resume_doc)\n",
        "# chat = ChatOpenAI(temperature=0.0, model=turbo_llm_memory)\n",
        "response = memory_llm_conversation(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X80kqGebMsO-",
        "outputId": "d99e4b7f-b6cd-48ab-9cb8-83f6961de775"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Not much, just hanging\n",
            "AI: Cool\n",
            "Human: [HumanMessage(content='For the following text, extract the following information:\\n\\nSkills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\\n\\nEducation: What is the highest education of the candidate and what is the GPA as mentioned in the text?Answer Output should be the university/college name and GPA if given in text, output them as a comma separated Python list.\\n\\nProjects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\\n\\nPublications: Extract all publication titles mentioned in a textand output them as a comma separated Python list.\\n\\nWork experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\\n\\nFormat the output as JSON with the following keys:\\nSkills\\nEducation\\nProjects\\nPublications\\nWork experience\\n\\ntext: ENGINEERING INTERN Skills\\n\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\n\\nExperience ENGINEERING INTERN 08/2016 ï¼\\u200b 12/2016 Company Name State\\n')]\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, resume_doc assigned to text is a document retrieved in previous steps based on the job description."
      ],
      "metadata": {
        "id": "21mMhFGIXqj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.keys())\n",
        "print(response['response'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jo0Bt6jmM5lB",
        "outputId": "bfbeca76-812c-4d2a-98eb-5a17560acc0e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input', 'history', 'response'])\n",
            "I can extract the information for you. Here is the output in JSON format:\n",
            "\n",
            "{\n",
            "  \"Skills\": [\"C++\", \"Python\", \"MATLAB\", \"Git\", \"Bash\", \"R\", \"SQL\", \"Linux/Unix\", \"Machine Learning Tools\", \"Computational Fluid Dynamics\", \"Discrete Element Method Codes\", \"Reservoir and Fracture Modeling Tools\", \"Experimental and Statistical Methods\"],\n",
            "  \"Education\": null,\n",
            "  \"Projects\": [\"XGBoost model with 77.5% accuracy in the Kaggle Titanic challenge\"],\n",
            "  \"Publications\": null,\n",
            "  \"Work experience\": [\"Company Name State, 08/2016 - 12/2016, ENGINEERING INTERN\"]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* recheck above if it is different from pdf\n",
        "* Parsing the LLM output using output parser\n",
        "Define response schemas for skills, projects and work experience that need to be extracted, using which create structured output parser.\n",
        "\n",
        "Create a prompt template to instruct to extract skills, projects and work experience, which also include placeholder for context(I.e resume in our case) and format instructions(derived from StructuredOutputParser).\n",
        "\n",
        "Now send the formatted prompt template to LLM whose output will be dictionary thus we can easily retrieve required answers using specific keys."
      ],
      "metadata": {
        "id": "BQP1zD6zXxqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "skills_schema = ResponseSchema(name=\"Skills\",\n",
        "                             description=\"what are the technical and non technical skills? \\\n",
        "Answer output them as a comma separated Python list.\")\n",
        "\n",
        "\n",
        "\n",
        "Projects_schema = ResponseSchema(name=\"Projects\",\n",
        "                                    description=\"Extract all project titles mentioned in a text\\\n",
        "and output them as a comma separated Python list.\")\n",
        "\n",
        "\n",
        "Work_experience_schema = ResponseSchema(name=\"Work experience\",\n",
        "                                    description=\"Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
        "and output them as a comma separated Python list.\")\n",
        "\n",
        "response_schemas = [skills_schema,\n",
        "\n",
        "                    Projects_schema,\n",
        "\n",
        "                   Work_experience_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVcb0Q0CNKFK",
        "outputId": "319d46ad-4746-4e4a-da98-150d18d8c901"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"Skills\": string  // what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
            "\t\"Projects\": string  // Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
            "\t\"Work experience\": string  // Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "review_template_2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "Skills: what are the technical and non technical skills? \\\n",
        "Answer output them as a comma separated Python list.\n",
        "\n",
        "Projects: Extract all project titles mentioned in a text\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designation\\\n",
        "and output them as a comma separated Python list.\n",
        "\n",
        "Format the output as JSON with the following keys:\n",
        "Skills\n",
        "Projects\n",
        "Work experience\n",
        "\n",
        "text: {text}\n",
        "\n",
        "{format_instructions}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
        "\n",
        "messages = prompt.format_messages(text=resume_doc,\n",
        "                                format_instructions=format_instructions)\n",
        "print(messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxTURzI9NWNA",
        "outputId": "22ec3c7b-919a-4a6f-fa7c-7f78d515ac54"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For the following text, extract the following information:\n",
            "\n",
            "Skills: what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
            "\n",
            "Projects: Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
            "\n",
            "Work experience: Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
            "\n",
            "Format the output as JSON with the following keys:\n",
            "Skills\n",
            "Projects\n",
            "Work experience\n",
            "\n",
            "text: ENGINEERING INTERN Skills\n",
            "\n",
            "C++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\n",
            "\n",
            "Experience ENGINEERING INTERN 08/2016 ï¼​ 12/2016 Company Name State\n",
            "\n",
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"Skills\": string  // what are the technical and non technical skills? Answer output them as a comma separated Python list.\n",
            "\t\"Projects\": string  // Extract all project titles mentioned in a textand output them as a comma separated Python list.\n",
            "\t\"Work experience\": string  // Extract all organisation name where he/she has worked along with number of years or months worked there and also extract designationand output them as a comma separated Python list.\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = turbo_llm_memory(messages)\n",
        "\n",
        "print(response2.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6SSK_bgNiuM",
        "outputId": "d51ccc86-a89c-4b81-cbb5-704e3d0e29c0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\t\"Skills\": \"C++, Python, MATLAB, Git, Bash, R, SQL (basic), Linux/Unix, high performance computing clusters, Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic), XGBoost, Computational Fluid Dynamics, Discrete Element Method Codes, CFD-DEM, OpenFOAM, CFD-ACE+, Fluent, COMSOL, LAMMPS, LIGGGHTS, Reservoir Modeling Tools, Fracture Modeling Tools, CMG, FracPro, Saphir, Experimental Methods, Statistical Methods, SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments\"\n",
            "\t\"Projects\": \"Kaggle Titanic challenge\"\n",
            "\t\"Work experience\": \"Company Name, 4 months, ENGINEERING INTERN\"\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Demonstrating Sequential Chains :**\n",
        "\n",
        "First chain is to extract technical and non technical skills.\n",
        "\n",
        "Second chain is for what are the job roles among Data Scientist, Machine learning Engineer, Software Engineer, Data Engineer, Devops Engineer, Cloud Architect. Are suited based on the given skill sets.\n",
        "\n",
        "Third chain is for explaining each skill as for what kind of projects are these useful."
      ],
      "metadata": {
        "id": "Tm5RT-nXYMsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.9, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Skills: what are the technical and non technical skills? \\\n",
        "Answer output them as a comma separated Python list.\"\n",
        "    \"\\n\\n{resume_doc}\"\n",
        ")\n",
        "\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt,\n",
        "                     output_key=\"skills\"\n",
        "                    )\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you name what the job roles among Data Scientist, Machine learning Engineer, Software Engineer, Data Engineer, Devops Engineer, Cloud Architect. Are suited based on the given skill sets\"\n",
        "    \"\\n\\n{skills}\"\n",
        ")\n",
        "\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt,\n",
        "                     output_key=\"job_titles\"\n",
        "                    )\n",
        "\n",
        "\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Explain each skill as for what kind of projects are these usefull:\\n\\n{skills}\"\n",
        ")\n",
        "# chain 3: input= Review and output= language\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
        "                       output_key=\"skills_explanation\"\n",
        "                      )\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three],\n",
        "    input_variables=[\"resume_doc\"],\n",
        "    output_variables=[\"skills\", \"job_titles\",\"skills_explanation\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "seqchain_output = overall_chain(resume_doc)\n",
        "seqchain_output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3U0atSRN4v8",
        "outputId": "e52fcf31-f339-4fe6-bf18-211949261a98"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'resume_doc': 'ENGINEERING INTERN Skills\\n\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\n\\nExperience ENGINEERING INTERN 08/2016 ï¼\\u200b 12/2016 Company Name State',\n",
              " 'skills': \"['C++', 'Python', 'MATLAB', 'Git', 'Bash', 'R', 'SQL', 'Linux/Unix', 'High performance computing clusters', 'Scikit-learn', 'Pandas', 'Seaborn', 'matplotlib', 'TensorFlow', 'XGBoost', 'Computational Fluid Dynamics', 'Discrete Element Method', 'CFD-DEM', 'OpenFOAM', 'CFD-ACE+', 'Fluent', 'COMSOL', 'LAMMPS', 'LIGGGHTS', 'Reservoir Modeling', 'Fracture Modeling', 'CMG', 'FracPro', 'Saphir', 'Experimental Methods', 'Statistical Methods', 'SEM', 'AFM', 'Confocal Microscopy', 'Regression analysis', 'Statistical process control', 'Design of experiments']\",\n",
              " 'job_titles': 'Based on the given skill sets, the job roles that are suited for each are as follows:\\n\\n1. Data Scientist:\\n- Python\\n- R\\n- SQL\\n- Scikit-learn\\n- Pandas\\n- Seaborn\\n- Matplotlib\\n- TensorFlow\\n- XGBoost\\n- Statistical Methods\\n- Regression analysis\\n- Statistical process control\\n- Design of experiments\\n\\n2. Machine Learning Engineer:\\n- Python\\n- Scikit-learn\\n- TensorFlow\\n- XGBoost\\n\\n3. Software Engineer:\\n- C++\\n- Python\\n- MATLAB\\n- Git\\n- Bash\\n- Linux/Unix\\n\\n4. Data Engineer:\\n- Python\\n- SQL\\n\\n5. Devops Engineer:\\n- Git\\n- Bash\\n- Linux/Unix\\n\\n6. Cloud Architect:\\n- High performance computing clusters\\n\\nEach job role may require a combination of these skills, and the above list is not exhaustive.',\n",
              " 'skills_explanation': '- C++: Useful for projects involving software development, performance-critical applications, system programming, and building high-performance applications.\\n- Python: Useful for a wide range of projects including web development, data analysis, machine learning, and automation tasks.\\n- MATLAB: Useful for projects in numerical computing, data analysis, signal processing, and image processing.\\n- Git: Useful for version control in software development projects, collaboration with team members, and managing code changes.\\n- Bash: Useful for scripting and automation tasks in Linux/Unix environments.\\n- R: Useful for statistical analysis, data visualization, and building statistical models.\\n- SQL: Useful for database management, querying, and data manipulation tasks.\\n- Linux/Unix: Useful for projects involving system administration, shell scripting, and server management.\\n- High performance computing clusters: Useful for running computationally intensive tasks and simulations that require high processing power.\\n- Scikit-learn: Useful for machine learning projects and building predictive models.\\n- Pandas: Useful for data manipulation and analysis tasks in Python.\\n- Seaborn, matplotlib: Useful for data visualization and plotting in Python.\\n- TensorFlow, XGBoost: Useful for projects in deep learning, machine learning, and building predictive models.\\n- Computational Fluid Dynamics (CFD): Useful for simulating fluid flow and heat transfer in engineering projects.\\n- Discrete Element Method (DEM): Useful for simulating granular materials and particle interactions in engineering projects.\\n- CFD-DEM, OpenFOAM, Fluent, COMSOL, LAMMPS, LIGGGHTS: Useful for specific CFD and DEM simulation projects in various software platforms.\\n- Reservoir Modeling, Fracture Modeling: Useful for projects in reservoir engineering and geomechanics.\\n- CMG, FracPro, Saphir: Useful for reservoir modeling and simulation software for oil and gas industry projects.\\n- Experimental Methods: Useful for conducting laboratory experiments, collecting data, and validating simulations.\\n- Statistical Methods, Regression analysis: Useful for data analysis, hypothesis testing, and building predictive models.\\n- Statistical process control, Design of experiments: Useful for quality control, process optimization, and experimentation in manufacturing and engineering projects.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demonstrating LangChain Tools and Agents**\n",
        "\n",
        "Agents : The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hard coded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.\n",
        "\n",
        "Tools: are interfaces that an agent can use to interact with the world.\n",
        "\n",
        "Here we define one custom tool to return job descriptions, other inbuilt tools which I used was llm-math and Wikipedia.\n",
        "\n",
        "query: what are the technical and non technical skills?"
      ],
      "metadata": {
        "id": "lc3aeyfpZVVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_experimental.agents.agent_toolkits import create_python_agent\n",
        "from langchain.agents import load_tools, initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.python import PythonREPL\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.agents import tool\n",
        "\n",
        "@tool\n",
        "def job_desription(text: str)-> str:\n",
        " \"\"\"Returns job disriptions mentioned below, use this for any \\\n",
        " questions related to knowing the job disription. \\\n",
        " The input should always be an empty string, \\\n",
        " and this function will always return a string containing job disriptions.\\ \"\"\"\n",
        "\n",
        " return \"Job discriptions:\\\n",
        " 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion.\\\n",
        " 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\"\n",
        "\n",
        "\n",
        "tools = load_tools([\"llm-math\",\"wikipedia\"], llm=turbo_llm)\n",
        "\n",
        "agent= initialize_agent(\n",
        "    tools+ [job_desription],\n",
        "    turbo_llm,  #turbo_llm, qa_chain,\n",
        "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    handle_parsing_errors=True,\n",
        "    verbose = True)\n",
        "\n",
        "agent_template = \"\"\"\\\n",
        "The following is the resume and query:\n",
        "\n",
        "resume: {resume}\n",
        "\n",
        "query: {query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=agent_template)\n",
        "query_human = 'Skills: what are the technical and non technical skills? \\Answer output them as a comma separated Python list.'\n",
        "messages = prompt.format_messages(resume=resume_doc,\n",
        "                                query=query_human)\n",
        "\n",
        "messages\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t43J-M8YbnT",
        "outputId": "d0c4a8d4-a9e7-40cb-c171-cf6e80bd0923"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='The following is the resume and query:\\n\\nresume: ENGINEERING INTERN Skills\\n\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\n\\nExperience ENGINEERING INTERN 08/2016 ï¼\\u200b 12/2016 Company Name State\\n\\nquery: Skills: what are the technical and non technical skills? \\\\Answer output them as a comma separated Python list.\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# query: Give me the available job descriptions?"
      ],
      "metadata": {
        "id": "JJlzDVEaaOmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_template = \"\"\"\\\n",
        "The following is the resume and query:\n",
        "\n",
        "resume: {resume}\n",
        "\n",
        "query: {query}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template=agent_template)\n",
        "query_human = 'Give me the available job discriptions?'\n",
        "messages = prompt.format_messages(resume=resume_doc,\n",
        "                                query=query_human)\n",
        "\n",
        "result = agent(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UukKHiIEZfTk",
        "outputId": "4b074366-17a7-43eb-8027-90975c7e5dab"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought: The user is asking for available job descriptions based on the skills mentioned in the resume. I can use the `job_desription` tool to provide relevant job descriptions.\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"job_desription\",\n",
            "  \"action_input\": \"\"\n",
            "}\n",
            "```\n",
            "\u001b[0m\n",
            "Observation: \u001b[38;5;200m\u001b[1;3mJob discriptions: 1)Machine learning Engineer:Machine Learning Engineer with expertise in designing and developing robust models and algorithms to solve complex business problems. Experienced in end-to-end machine learning pipelines, from data preprocessing to deployment. Proficient in Python, TensorFlow, and PyTorch. Skilled in data preprocessing, feature engineering, and cloud platforms (AWS, Azure, GCP). Strong communicator with a collaborative approach and a proven ability to drive projects to completion. 2) Computer Vision Engineer:Computer Vision Engineer specializing in 3D scan structure extraction and model development. Collaborates with product and research teams to enhance current products and enable new ones. Experienced with massive datasets, 2D Deep Learning, and Computer Vision using PyTorch and/or TensorFlow. Balances generalist and researcher roles, ensuring ML models transition into meaningful production. Works closely with product owners to deliver value efficiently to customers.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe job descriptions provided are for a Machine Learning Engineer and a Computer Vision Engineer based on the skills mentioned in the resume.\n",
            "Final Answer: The available job descriptions are for a Machine Learning Engineer and a Computer Vision Engineer.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Demonstrating Multi Prompt chain**\n",
        "\n",
        "Demonstrating use the RouterChain paradigm to create a chain that dynamically selects the prompt to use for a given input. Specifically use the MultiPromptChain to create a question-answering chain that selects the prompt which is most relevant for a given question, and then answers the question using that prompt"
      ],
      "metadata": {
        "id": "qrEhzwX9a9la"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define 3 prompt template:\n",
        "\n",
        "## Job description:"
      ],
      "metadata": {
        "id": "IoKCEo18bQEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"You are good at matching available job description with resume.\\\n",
        "Steps:\\\n",
        "1.Retrieve job descriptions from given tool attached with agent \\\n",
        "2.Compare if resume can be selected based on any job description, if yes then return that specific job description\n",
        "3.If no job description matches the return None\n",
        "Here is a resume:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "id": "6dl1YSuAbYY3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "1cc2468b-7520-4128-f1af-34f81859fd83"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are good at matching available job description with resume.Steps:1.Retrieve job descriptions from given tool attached with agent 2.Compare if resume can be selected based on any job description, if yes then return that specific job description\\n3.If no job description matches the return None\\nHere is a resume:\\n{input}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portfolio :"
      ],
      "metadata": {
        "id": "v56RuWc0bpgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You are good at finding portfolio link from the given resume and return that link to the user.If link not found return None.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "v41fCbvAbqlg",
        "outputId": "7e4673c5-12c6-4181-93f8-4b5ab0a1b6dd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYou are good at finding portfolio link from the given resume and return that link to the user.If link not found return None.\\n\\nHere is a question:\\n{input}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:"
      ],
      "metadata": {
        "id": "9zXAs_1Rbvy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You are good at summerising the given resume. You will include skills, professional experience, education in the summary.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DfsM7ZJSbsWe",
        "outputId": "378378db-4995-4fd4-95dc-52738d2c787c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nYou are good at summerising the given resume. You will include skills, professional experience, education in the summary.\\n\\nHere is a question:\\n{input}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job_description_template = \"\"\"\n",
        "You are good at matching available job description with resume.\\\n",
        "Steps:\\\n",
        "1.Retreive job discriptions from givel tool attached with agent \\\n",
        "2.Compare if resume can be selected based on any job discription, if yes then retuen that specific job discription\n",
        "3.If no job discription matches the return None\n",
        "\n",
        "Here is a resume:\n",
        "{input}\"\"\"\n",
        "\n",
        "\n",
        "portfolio_finder_template = \"\"\"\n",
        "\n",
        "You are good at finding portfolio link from the given resume and return that link to the user.If link not found return None.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "summary_template = \"\"\"\n",
        "You are good at summerising the given resume. You will include skills, professional experience, education in the summary.\n",
        "\n",
        "Here is a question:\n",
        "{input}\"\"\"\n",
        "\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"job_description\",\n",
        "        \"description\": \"Good for providing job discription that is matched\",\n",
        "        \"prompt_template\": job_description_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"portfolio\",\n",
        "        \"description\": \"Good for returning portfolio link from resume\",\n",
        "        \"prompt_template\": portfolio_finder_template\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"summary\",\n",
        "        \"description\": \"Good for providing summary of resume\",\n",
        "        \"prompt_template\": summary_template\n",
        "    }\n",
        "]\n"
      ],
      "metadata": {
        "id": "5dNqBp-lbykj"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create destination chains:"
      ],
      "metadata": {
        "id": "ZlHokfPyb6IW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
        "\n",
        "destination_chains = {}\n",
        "for p_info in prompt_infos:\n",
        "    name = p_info[\"name\"]\n",
        "    prompt_template = p_info[\"prompt_template\"]\n",
        "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
        "    if name == \"job_description\":\n",
        "        chain = agent\n",
        "    elif name == \"portfolio\" :\n",
        "        chain = LLMChain(llm=llm, prompt=prompt)\n",
        "    else:\n",
        "        chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    destination_chains[name] = chain\n",
        "\n",
        "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
        "destinations_str = \"\\n\".join(destinations)\n",
        "destinations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRsqxoLbb3zs",
        "outputId": "ce7a1849-6f69-4040-db43-036ffc5efe11"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['job_description: Good for providing job discription that is matched',\n",
              " 'portfolio: Good for returning portfolio link from resume',\n",
              " 'summary: Good for providing summary of resume']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define multi prompt router prompt and multi prompt chain:"
      ],
      "metadata": {
        "id": "mzRR0fkscM7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
        "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
        "\n",
        "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
        "language model select the model prompt best suited for the input. \\\n",
        "You will be given the names of the available prompts and a \\\n",
        "description of what the prompt is best suited for. \\\n",
        "You may also revise the original input if you think that revising\\\n",
        "it will ultimately lead to a better response from the language model.\n",
        "\n",
        "<< FORMATTING >>\n",
        "Return a markdown code snippet with a JSON object formatted to look like:\n",
        "```json\n",
        "{{{{\n",
        "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
        "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
        "}}}}\n",
        "```\n",
        "\n",
        "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
        "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
        "well suited for any of the candidate prompts.\n",
        "REMEMBER: \"next_inputs\" can just be the original input \\\n",
        "if you don't think any modifications are needed.\n",
        "\n",
        "<< CANDIDATE PROMPTS >>\n",
        "{destinations}\n",
        "\n",
        "<< INPUT >>\n",
        "{{input}}\n",
        "\n",
        "<< OUTPUT (remember to include the ```json)>>\"\"\"\n",
        "\n",
        "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
        "    destinations=destinations_str\n",
        ")\n",
        "router_prompt = PromptTemplate(\n",
        "    template=router_template,\n",
        "    input_variables=[\"input\"],\n",
        "    output_parser=RouterOutputParser(),\n",
        ")\n",
        "\n",
        "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
        "\n",
        "chain = MultiPromptChain(router_chain=router_chain,\n",
        "                         destination_chains=destination_chains,\n",
        "                         default_chain=default_chain, verbose=True\n",
        "                        )\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7p3Spj8b_kw",
        "outputId": "b96a059f-818a-4513-8840-cf630d825994"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultiPromptChain(verbose=True, router_chain=LLMRouterChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['input'], output_parser=RouterOutputParser(), template='Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revisingit will ultimately lead to a better response from the language model.\\n\\n<< FORMATTING >>\\nReturn a markdown code snippet with a JSON object formatted to look like:\\n```json\\n{{\\n    \"destination\": string \\\\ name of the prompt to use or \"DEFAULT\"\\n    \"next_inputs\": string \\\\ a potentially modified version of the original input\\n}}\\n```\\n\\nREMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is notwell suited for any of the candidate prompts.\\nREMEMBER: \"next_inputs\" can just be the original input if you don\\'t think any modifications are needed.\\n\\n<< CANDIDATE PROMPTS >>\\njob_description: Good for providing job discription that is matched\\nportfolio: Good for returning portfolio link from resume\\nsummary: Good for providing summary of resume\\n\\n<< INPUT >>\\n{input}\\n\\n<< OUTPUT (remember to include the ```json)>>'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f603d81d540>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f603d81e9e0>, temperature=0.0, openai_api_key='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', openai_proxy=''))), destination_chains={'job_description': AgentExecutor(verbose=True, tags=['chat-zero-shot-react-description'], agent=ChatAgent(llm_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Answer the following questions as best you can. You have access to the following tools:\\n\\nCalculator: Useful for when you need to answer questions about math.\\nwikipedia: A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.\\njob_desription: job_desription(text: str) -> str - Returns job disriptions mentioned below, use this for any  questions related to knowing the job disription.  The input should always be an empty string,  and this function will always return a string containing job disriptions.\\\\\\n\\nThe way you use the tools is by specifying a json blob.\\nSpecifically, this json should have a `action` key (with the name of the tool to use) and a `action_input` key (with the input to the tool going here).\\n\\nThe only values that should be in the \"action\" field are: Calculator, wikipedia, job_desription\\n\\nThe $JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. Here is an example of a valid $JSON_BLOB:\\n\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\n\\nALWAYS use the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: the result of the action\\n... (this Thought/Action/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin! Reminder to always use the exact characters `Final Answer` when responding.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], template='{input}\\n\\n{agent_scratchpad}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f6046e29db0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f6046e281f0>, temperature=0.0, openai_api_key='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', openai_proxy='')), output_parser=ChatOutputParser(), allowed_tools=['Calculator', 'wikipedia', 'job_desription']), tools=[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f6046e29db0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f6046e281f0>, temperature=0.0, openai_api_key='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', openai_proxy='')))>, coroutine=<bound method Chain.arun of LLMMathChain(llm_chain=LLMChain(prompt=PromptTemplate(input_variables=['question'], template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f6046e29db0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f6046e281f0>, temperature=0.0, openai_api_key='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', openai_proxy='')))>), WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from '/usr/local/lib/python3.10/dist-packages/wikipedia/__init__.py'>, top_k_results=3, lang='en', load_all_available_meta=False, doc_content_chars_max=4000)), StructuredTool(name='job_desription', description='job_desription(text: str) -> str - Returns job disriptions mentioned below, use this for any  questions related to knowing the job disription.  The input should always be an empty string,  and this function will always return a string containing job disriptions.\\\\', args_schema=<class 'pydantic.v1.main.job_desriptionSchema'>, func=<function job_desription at 0x7f603d9741f0>)], handle_parsing_errors=True), 'portfolio': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\n\\nYou are good at finding portfolio link from the given resume and return that link to the user.If link not found return None.\\n\\nHere is a question:\\n{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f603d81d540>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f603d81e9e0>, temperature=0.0, openai_api_key='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', openai_proxy='')), 'summary': LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='\\nYou are good at summerising the given resume. You will include skills, professional experience, education in the summary.\\n\\nHere is a question:\\n{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f603d81d540>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f603d81e9e0>, temperature=0.0, openai_api_key='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', openai_proxy=''))}, default_chain=LLMChain(prompt=ChatPromptTemplate(input_variables=['input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))]), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f603d81d540>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f603d81e9e0>, temperature=0.0, openai_api_key='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', openai_proxy='')))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let’s define a input prompt to for extracting portfolio link from resume:"
      ],
      "metadata": {
        "id": "XbAeMg0OcaOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_template1 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "Portfolio link: Extract portfolio link from given document.\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template1 = ChatPromptTemplate.from_template(review_template1)\n",
        "messages1 = prompt_template1.format_messages(text=resume_doc[:]) #resume_doc[300:] to test portfolio link :None\n",
        "print(prompt_template)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah-Cn6abcUMH",
        "outputId": "890a04e4-935d-44eb-dd23-9563b4eef8a7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are good at summerising the given resume. You will include skills, professional experience, education in the summary.\n",
            "\n",
            "Here is a question:\n",
            "{input}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s define a input prompt to give matching job_description, if nothing matches give None:"
      ],
      "metadata": {
        "id": "1AgfHyrjciP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review_template2 = \"\"\"\\\n",
        "For the following text, extract the following information:\n",
        "\n",
        "Give matching job_description, if nothing matches give None.\n",
        "\n",
        "text: {text}\n",
        "\"\"\"\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template1 = ChatPromptTemplate.from_template(review_template2)\n",
        "messages1 = prompt_template1.format_messages(text=resume_doc[:]) #resume_doc[300:] to test portfolio link :None\n",
        "res=chain.run(messages1)\n",
        "res"
      ],
      "metadata": {
        "id": "5QPBObUucehy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2RgworUZdeq",
        "outputId": "857d5bf3-3caf-4051-dbf0-9ec87d2e87e0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='For the following text, extract the following information:\\n\\nGive matching job_description, if nothing matches give None.\\n\\ntext: ENGINEERING INTERN Skills\\n\\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters. Machine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost model that has 77.5% accuracy in the Kaggle Titanic challenge.) Computational Fluid Dynamics and Discrete Element Method Codes CFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS. Reservoir and Fracture Modeling Tools CMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis. Experimental and Statistical Methods SEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\\n\\nExperience ENGINEERING INTERN 08/2016 ï¼\\u200b 12/2016 Company Name State\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not let’s create a response schema to extract portfolio link:"
      ],
      "metadata": {
        "id": "M_WVWLm3cwrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.output_parsers import ResponseSchema\n",
        "from langchain.output_parsers import StructuredOutputParser\n",
        "\n",
        "portfolio_link_schema = ResponseSchema(name=\"portfolio_link\",\n",
        "                             description=\"Give portfolio link from the given resume and return that link to the user\\\n",
        "Answer output them as a comma separated Python list.\")\n",
        "\n",
        "response_schemas = [portfolio_link_schema]\n",
        "\n",
        "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
        "\n",
        "format_instructions = output_parser.get_format_instructions()\n",
        "\n",
        "print(format_instructions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FTHoif7cqNJ",
        "outputId": "c69fcde9-b733-4931-c131-eef0898f2da7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
            "\n",
            "```json\n",
            "{\n",
            "\t\"portfolio_link\": string  // Give portfolio link from the given resume and return that link to the userAnswer output them as a comma separated Python list.\n",
            "}\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nb96OGvvZ9fZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}